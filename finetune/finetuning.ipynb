{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e9937f-8551-4244-a72b-9e4d956b53dc",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06cbb181-8a40-4ecf-a198-0d6ddd3c4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from datasets import load_dataset\n",
    "from FlagEmbedding import FlagModel\n",
    "from FlagEmbedding.baai_general_embedding.finetune.hn_mine import find_knn_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9f8e73-62e2-4b90-b8b8-ef66fc1670b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl_file(file, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for item in file:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "    print(f\"Save at {path}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99048fea-36b8-4b92-bee7-bd76e1e39334",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b415de0d-860d-4c61-b609-f794ffd5d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"airesearch/WangchanX-Legal-ThaiCCL-RAG\"\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "output_dir = 'outputs'\n",
    "temporary_dir = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ba5b23-6d95-48f9-a7e3-b84bad5de1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "if not os.path.exists(temporary_dir):\n",
    "    os.makedirs(temporary_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76c982-ac46-47bc-b0a8-60b92af4f213",
   "metadata": {},
   "source": [
    "## Load data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5f4c24-c88f-4b7b-8ccf-eccb8eb2c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d11b8-dd22-425b-9292-e8f59e26109a",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf28163-7caa-4e6f-96e5-aa8bf5bbc95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.legal documents = 4513\n"
     ]
    }
   ],
   "source": [
    "# legal documents\n",
    "legal_documents = []\n",
    "for split in legal_dataset.keys():\n",
    "    for data in legal_dataset[split]:\n",
    "        legal_documents += [i['text'] for i in data['positive_contexts'] if len(i['text']) != 0]\n",
    "\n",
    "legal_documents = sorted(list(set(legal_documents)))\n",
    "print(f'No.legal documents = {len(legal_documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "118f1ec3-1676-44be-8e36-7e9f1bb5b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at temp/temp_positive_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# positive data\n",
    "positive_data = []\n",
    "for data in legal_dataset['train']:\n",
    "    pos = [i['text'] for i in data['positive_contexts'] if len(i['text']) != 0]\n",
    "    if len(pos) != 0:\n",
    "        positive_data.append({'query': data['question'], 'pos': pos})\n",
    "\n",
    "temp_input_path = os.path.join(temporary_dir, 'temp_positive_data.jsonl')\n",
    "save_jsonl_file(positive_data, temp_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca8b1e-6475-4f8d-8aee-be4a103f6655",
   "metadata": {},
   "source": [
    "### Hard Negatives\n",
    "Hard negatives is a widely used method to improve the quality of sentence embedding. You can mine hard negatives following this command:\n",
    "\n",
    "- `input_file`: json data for finetuning. \n",
    "- `output_file`: path to save JSON data with mined hard negatives for finetuning\n",
    "- `negative_number`: the number of sampled negatives \n",
    "- `sample_range`: where to sample negative. For example, `[2, 100]` means sampling `negative_number` negatives from top2-top200 documents. **You can set larger value to reduce the difficulty of negatives (e.g., set it `[60, 300]` to sample negatives from top60-300 passages)**\n",
    "- `candidate_pool`: The pool to retrieval. The default value is None, and this script will retrieve from the combination of all `neg` in `input_file`. \n",
    "The format of this file is the same as [pretrain data](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain#2-data-format). If input a candidate_pool, this script will retrieve negatives from this file.\n",
    "- `use_gpu_for_searching`: whether to use faiss-gpu to retrieve negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b6f1c6-688b-448f-bcb1-bbc5076370d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 4*GPUs----------\n",
      "inferencing embedding for corpus (number=4513)--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|███████████████████████| 5/5 [00:06<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencing embedding for queries (number=7238)--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|███████████████████████| 8/8 [00:01<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create index and search------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████| 114/114 [00:04<00:00, 27.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# hard negative data for fine tuning\n",
    "temp_hn_path = os.path.join(temporary_dir, 'temp_hn_data.jsonl')\n",
    "model = FlagModel(model_name, query_instruction_for_retrieval=\"\")\n",
    "find_knn_neg(\n",
    "    model=model, \n",
    "    input_file=temp_input_path, \n",
    "    candidate_pool=legal_documents, \n",
    "    output_file=temp_hn_path, \n",
    "    sample_range=[2, 100], \n",
    "    negative_number=3, \n",
    "    use_gpu=False\n",
    ")\n",
    "os.remove(temp_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6942c-4307-4ee8-a8c1-965e49f325f8",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here is an simple example of how to perform unified fine-tuning (dense embedding, sparse embedding and colbert) based on `BAAI/bge-m3`\n",
    "\n",
    "**some important arguments**:\n",
    "- `per_device_train_batch_size`: batch size in training. In most of cases, larger batch size will bring stronger performance.\n",
    "- `train_group_size`: the number of positive and negatives for a query in training.\n",
    "There are always one positive, so this argument will control the number of negatives (#negatives=train_group_size-1).\n",
    "Noted that the number of negatives should not be larger than the numbers of negatives in data `\"neg\":List[str]`.\n",
    "Besides the negatives in this group, the in-batch negatives also will be used in fine-tuning.\n",
    "- `negatives_cross_device`: share the negatives across all GPUs. This argument will extend the number of negatives.\n",
    "- `learning_rate`: select a appropriate for your model. Recommend 1e-5/2e-5/3e-5 for large/base/small-scale. \n",
    "- `temperature`: It will influence the distribution of similarity scores. **Recommended value: 0.01-0.1.**\n",
    "- `query_max_len`: max length for query. Please set it according the average length of queries in your data.\n",
    "- `passage_max_len`: max length for passage. Please set it according the average length of passages in your data.\n",
    "\n",
    "For more training arguments please refer to [transformers.TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7e060-43df-4319-9ea9-6031c09efad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [\n",
    "    \"cd FlagEmbedding\",\n",
    "    f\"\"\"\n",
    "    CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node 1 \\\n",
    "        -m FlagEmbedding.BGE_M3.run \\\n",
    "        --output_dir {output_dir} \\\n",
    "        --model_name_or_path {model_name} \\\n",
    "        --train_data {temporary_dir} \\\n",
    "        --learning_rate 1e-5 \\\n",
    "        --num_train_epochs 5 \\\n",
    "        --per_device_train_batch_size 1 \\\n",
    "        --dataloader_drop_last True \\\n",
    "        --normlized True \\\n",
    "        --temperature 0.02 \\\n",
    "        --query_max_len 64 \\\n",
    "        --passage_max_len 256 \\\n",
    "        --train_group_size 4 \\\n",
    "        --negatives_cross_device \\\n",
    "        --logging_steps 1000 \\\n",
    "        --same_task_within_batch True \\\n",
    "        --unified_finetuning True \\\n",
    "        --use_self_distill True \\\n",
    "        --save_strategy epoch \\\n",
    "    \"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ca87f-6215-4cf3-90d6-2b1d1d9df8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run commands\n",
    "for command in commands:\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37758110-6534-4ba3-9dd3-05e15e8b6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temporary_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
